{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science With scikit-learn\n",
    "\n",
    "### Overview\n",
    "\n",
    "Here we're going to walk through running a model, and looking at the results. Before we get started let's go over some terminology...\n",
    "\n",
    "### Terminology\n",
    "\n",
    "1.) `features` - Another word for the X variables, independent variables, predictors, regressors...   \n",
    "2.) `target` - Another word for the Y variable, dependent variable, outcome variable, response...  \n",
    "3.) `model` - What we use to relate one set of variables to another.      \n",
    "4.) `training data set` - Refers to the observations from your data set that are used to train/learn the statistical model.   \n",
    "5.) `testing data set` - Refers to the observations from your data that are **not** used to train/learn the statistical model. They are held out, and not seen by the model during training.  \n",
    "\n",
    "### Scikit-learn import\n",
    "\n",
    "```python\n",
    "import sklearn \n",
    "```\n",
    "\n",
    "Typically we're actually going to be importing something from one of the modules/libraries in `sklearn`. The [sklearn main page](http://scikit-learn.org/stable/) can help you determine where you might find something that you are looking for, and the [API reference](http://scikit-learn.org/stable/modules/classes) is also pretty helpful. A large majority of all of the machine learning algorithms you might run can be found somewhere within `sklearn`. Today we're going to talk through using a `Random Forest Regressor`.\n",
    "\n",
    "### General workflow\n",
    "\n",
    "Here are the steps by which we train a model... \n",
    "\n",
    "1.) Import whatever model you'll be fitting.  \n",
    "2.) Instantiate the model (i.e. create a variable that holds your model object). Set any hyperparameters as you see fit (we'll discuss what these are shortly).   \n",
    "3.) Feed in the X and Y variables (features and target) to the `.fit()` method.   \n",
    "4.) Call the `.score()` or `.predict()` method to see how well the model does on the training data (or new data). \n",
    "\n",
    "##### What would be another word/term we might use to describe the `new data` from step (4) above?\n",
    "\n",
    "We'll be working with a `RandomForestRegressor` tonight, which you can see the documentation for [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). I don't have time today to discuss in detail how the algorithm works, but I will if you take the data science course or ask me after class. It's well explained in the Elements of Statistical Learning, which you can download [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>91.4</td>\n",
       "      <td>142.4</td>\n",
       "      <td>601.4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>39</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>tue</td>\n",
       "      <td>88.8</td>\n",
       "      <td>147.3</td>\n",
       "      <td>614.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>66</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>tue</td>\n",
       "      <td>94.8</td>\n",
       "      <td>108.3</td>\n",
       "      <td>647.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>sep</td>\n",
       "      <td>sat</td>\n",
       "      <td>92.5</td>\n",
       "      <td>121.1</td>\n",
       "      <td>674.4</td>\n",
       "      <td>8.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>29</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>jan</td>\n",
       "      <td>sat</td>\n",
       "      <td>82.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>78</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>85.9</td>\n",
       "      <td>19.5</td>\n",
       "      <td>57.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>12.7</td>\n",
       "      <td>52</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>thu</td>\n",
       "      <td>91.4</td>\n",
       "      <td>30.7</td>\n",
       "      <td>74.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>29</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>90.2</td>\n",
       "      <td>99.6</td>\n",
       "      <td>631.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>21.4</td>\n",
       "      <td>33</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>sep</td>\n",
       "      <td>sat</td>\n",
       "      <td>92.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>698.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>45</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>sep</td>\n",
       "      <td>mon</td>\n",
       "      <td>88.6</td>\n",
       "      <td>91.8</td>\n",
       "      <td>709.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>17.4</td>\n",
       "      <td>56</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>85.9</td>\n",
       "      <td>19.5</td>\n",
       "      <td>57.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>13.7</td>\n",
       "      <td>43</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>sep</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>704.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>39</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>sep</td>\n",
       "      <td>mon</td>\n",
       "      <td>91.8</td>\n",
       "      <td>78.5</td>\n",
       "      <td>724.3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>18.9</td>\n",
       "      <td>35</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>tue</td>\n",
       "      <td>88.1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>67.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>15.8</td>\n",
       "      <td>27</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>mar</td>\n",
       "      <td>tue</td>\n",
       "      <td>88.1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>67.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>15.5</td>\n",
       "      <td>27</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>sat</td>\n",
       "      <td>91.7</td>\n",
       "      <td>35.8</td>\n",
       "      <td>80.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>11.6</td>\n",
       "      <td>30</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>sat</td>\n",
       "      <td>91.7</td>\n",
       "      <td>35.8</td>\n",
       "      <td>80.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>15.2</td>\n",
       "      <td>27</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>mar</td>\n",
       "      <td>mon</td>\n",
       "      <td>90.1</td>\n",
       "      <td>39.7</td>\n",
       "      <td>86.6</td>\n",
       "      <td>6.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>thu</td>\n",
       "      <td>93.0</td>\n",
       "      <td>75.3</td>\n",
       "      <td>466.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>36</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X  Y month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  area\n",
       "100  3  4   aug  sun  91.4  142.4  601.4  10.6  19.8  39   5.4   0.0   0.0\n",
       "101  3  4   aug  tue  88.8  147.3  614.5   9.0  14.4  66   5.4   0.0   0.0\n",
       "102  2  4   aug  tue  94.8  108.3  647.1  17.0  20.1  40   4.0   0.0   0.0\n",
       "103  2  4   sep  sat  92.5  121.1  674.4   8.6  24.1  29   4.5   0.0   0.0\n",
       "104  2  4   jan  sat  82.1    3.7    9.3   2.9   5.3  78   3.1   0.0   0.0\n",
       "105  4  5   mar  fri  85.9   19.5   57.3   2.8  12.7  52   6.3   0.0   0.0\n",
       "106  4  5   mar  thu  91.4   30.7   74.3   7.5  18.2  29   3.1   0.0   0.0\n",
       "107  4  5   aug  sun  90.2   99.6  631.2   6.3  21.4  33   3.1   0.0   0.0\n",
       "108  4  5   sep  sat  92.5   88.0  698.6   7.1  20.3  45   3.1   0.0   0.0\n",
       "109  4  5   sep  mon  88.6   91.8  709.9   7.1  17.4  56   5.4   0.0   0.0\n",
       "110  4  4   mar  fri  85.9   19.5   57.3   2.8  13.7  43   5.8   0.0   0.0\n",
       "111  3  4   mar  fri  91.7   33.3   77.5   9.0  18.8  18   4.5   0.0   0.0\n",
       "112  3  4   sep  sun  89.7   90.0  704.4   4.8  22.8  39   3.6   0.0   0.0\n",
       "113  3  4   sep  mon  91.8   78.5  724.3   9.2  18.9  35   2.7   0.0   0.0\n",
       "114  3  4   mar  tue  88.1   25.7   67.6   3.8  15.8  27   7.6   0.0   0.0\n",
       "115  3  5   mar  tue  88.1   25.7   67.6   3.8  15.5  27   6.3   0.0   0.0\n",
       "116  3  4   mar  sat  91.7   35.8   80.8   7.8  11.6  30   6.3   0.0   0.0\n",
       "117  3  4   mar  sat  91.7   35.8   80.8   7.8  15.2  27   4.9   0.0   0.0\n",
       "118  3  4   mar  mon  90.1   39.7   86.6   6.2  10.6  30   4.0   0.0   0.0\n",
       "119  3  4   aug  thu  93.0   75.3  466.6   7.7  19.6  36   3.1   0.0   0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/forestfires.csv') # Get the data.\n",
    "print(df.shape)\n",
    "df[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # Import our model. \n",
    "random_forest = RandomForestRegressor(n_estimators=100) # Instantiate it with 100 trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our features (X variables) and target (Y variable). I'm using the forest-fire \n",
    "data, and for now am only going to use the `X` and `Y` columns (which are the spatial coordinates of the fires) for the features, and the `area` column for the target (this is defined as the dependent variable on the UCI website where I got this data). A link to the data and it's description can be found [here](https://archive.ics.uci.edu/ml/datasets/Forest+Fires). As you can see, I'm building a model that predicts the burned area of forest fires in northeastern Portugal based on the precise geographical coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do I pull the X and Y columns from our df to use as the features? How about the area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = df[['X','Y']]\n",
    "target = df['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit/train the model (i.e. build the model based off the training data)\n",
    "random_forest.fit(features, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = random_forest.predict(features) # This gives us back a vector of predictions\n",
    "                                              # (one for each observation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of metrics, the [sklearn.metrics](http://scikit-learn.org/stable/modules/classes#sklearn-metrics-metrics) documentation will give you an idea of any of the metrics you can use to judge a model. The majority of these take the format of a fuction call where you input `(y_predictions, y_observations)`, and they output the calculated metric. We'll look at mean squared error below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3911.57239199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(predictions, target)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can add in something else and get better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3236.49155097\n"
     ]
    }
   ],
   "source": [
    "features = df[['X','Y', 'wind']]\n",
    "target = df['area']\n",
    "random_forest.fit(features, target)\n",
    "predictions = random_forest.predict(features)\n",
    "print(mean_squared_error(predictions, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have run two models now, and saw that the second one performed better. We could keep adding variables into our model, checking the MSE after adding in any variable. But we're not actually running our model on any data that we aren't training it on, so how do we know that what we are putting into our model would actually help on data that we've never seen. In other words, how do we tell if our model will generalize well? The answer is **cross validation**.\n",
    "\n",
    "The way that **cross validation** works is that we break our data into `k` number of folds (typically 5 or 10). We train our model on `k-1` of those folds, and then predict on the `kth` fold. We take those predictions, and then get our scoring metric (`mean squared error`, in our case) using those predictions. We then do this again, and again, and again, until each of the `k` folds has been used for predictions (so with 5 folds, we do this 5 times, and with 10 folds, 10 times, etc.)\n",
    "\n",
    "![cross-val-image](http://i.stack.imgur.com/1fXzJ.png)\n",
    "\n",
    "Using **cross validation**, we can get an idea of how our model would perform on data it hasn't seen before, and then when we add in variables into our model (or change model hyperparameters), we can be more sure that they were actually worth putting into our model.\n",
    "\n",
    "Best of all, it turns out that sklearn has a library we can use for this! Check out the [cross validation library](http://scikit-learn.org/stable/modules/classes#module-sklearn.cross_validation) for all the details. Today we'll be looking at the `cross_val_score` function, which allows you to pass in a model, a target (Y), a feature set (X), a number of folds (5 or 10, for example), and a scoring function (we'll use our mean_squared_error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   423.0957952 ,    158.79785535,    493.16636869,    510.14849252,\n",
       "           65.56792417,    269.31256367,    330.10878029,    182.42779078,\n",
       "         1928.7019805 ,  50507.00150498,    115.29716993,    551.15148581,\n",
       "          412.34427826,    136.63077743,   1215.61687166,  21388.19063418,\n",
       "         1554.61814158,    478.27897526,   3364.08126135,    359.08456373])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "features = df[['X','Y']]\n",
    "target = df['area']\n",
    "# cross_val_score flips the sign, so I take the negative below:\n",
    "results = -cross_val_score(random_forest, features, target, cv=20, scoring='mean_squared_error')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4222.1811607676964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5146.6889662686499"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['X', 'Y', 'wind']]\n",
    "target = df['area']\n",
    "results = -cross_val_score(random_forest, features, target, cv=20, scoring='mean_squared_error')\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like `wind` might not have been as helpful as we thought. Good think we used cross-validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a crucial part of a data-scientists workflow. We have to make sure that our model will generalize well, and cross-validation is a way to make sure that we are putting the right variables into our model. It can also be used to check our model hyperparameters (for a random forest, this might be the number of trees, the depth of each tree, etc.). `Sklearn` also has a built in to perform cross-validation over hyperparameters. It is located in the `sklearn.grid_search` module, and it is called `GridSearchCV`. As arguments, it takes an estimator/model (such as our Random Forest) and a parameter grid (dictionary). We instantiate it with these, and then we call the `.fit()` method on it, passing it our features and target. It returns back to us the best parameters to use for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "random_forest = RandomForestRegressor()\n",
    "param_grid = {'n_estimators': [10, 100, 500], 'max_depth': [1, 3, 5]}\n",
    "grid_search_cv = GridSearchCV(random_forest, param_grid, scoring='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 100, 500], 'max_depth': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='mean_squared_error',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['X', 'Y', 'wind']]\n",
    "target = df['area']\n",
    "grid_search_cv.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = grid_search_cv.best_estimator_ # Get a copy of the best model. \n",
    "best_params = grid_search_cv.best_params_ # Get a dictionary of the best parameters. \n",
    "best_score = grid_search_cv.best_score_ # Get the best score of scoring function we passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4154.9916400182738"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 100, 500], 'max_depth': [1, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='mean_squared_error',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['X', 'Y']]\n",
    "target = df['area']\n",
    "grid_search_cv.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = grid_search_cv.best_estimator_ # Get a copy of the best model. \n",
    "best_params = grid_search_cv.best_params_ # Get a dictionary of the best parameters. \n",
    "best_score = grid_search_cv.best_score_ # Get the best score of scoring function we passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4159.6908088064329"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's actually not that much of a difference between the performance of the best model, whether or not we include \"wind\" as a feature. In general (to avoid the risk of overfitting), simpler models should always be used unless the MSE improves significantly by adding additional features. In this case, adding \"wind\" actually makes the MSE go up, so it's clear we shouldn stick with the model that only uses geographical coordinates.\n",
    "\n",
    "With all this being said, we can kind of re-define/re-work the steps in our general workflow... \n",
    "\n",
    "1.) Import whatever model you'll be fitting.  \n",
    "2.) Instantiate the model (i.e. create a variable that holds your model object). Set any hyperparameters as you see fit.   \n",
    "3.) Feed in the X and Y variables (features and target) to the `.fit()` method.   \n",
    "4.) Call the `.score()` or `.predict()` method to see how well the model does on the training data (or new data).   \n",
    "5.) Repeat steps (2) - (4) to find the best model given your chosen scoring metric.\n",
    "\n",
    "**Note**: This assumes that all of your feature engineering/variable manipulation is done. Also note that the random forest is unique among machine learning models in that you don't have to cross-validate to test it (you can use out-of-box score instead), which is nice because cross-validation is computationally expensive and requires you to holdout data that could have been used to train the model. This is a discussion for another day though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Want some practice?\n",
    "\n",
    "Sign up for the data science bootcamp to get a more in-depth understanding of various stats/ML models and how and when to use them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
